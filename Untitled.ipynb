{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "673d2de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import string\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import ast\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e7a958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of a paper : \n",
    "\n",
    "class model(nn.Module): \n",
    "\n",
    "    def __init__(self, data, batch_size ,embedding_size, hidden_size,num_layers ,dropout, teacher_forcing_ratio, learning_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        '''\n",
    "        our input data are arabic words with their roots, with the hypothesis that each word has it's own root.\n",
    "        \n",
    "        (our dataset for this is named root_data)\n",
    "        '''\n",
    "        self.sow = '$'\n",
    "        self.eow = 'Â£'\n",
    "        self.lr = learning_rate\n",
    "        self.ratio = 0.9\n",
    "        self.batch_size = batch_size\n",
    "        self.data = data\n",
    "        self.batches, self.vocab, self.char_index_dic = self.prepare_data(self.data)\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_dense = nn.Linear(self.hidden_size * 2,self.embedding_size)\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.embedding = nn.Embedding(num_embeddings = len(self.vocab), embedding_dim = self.embedding_size, padding_idx = self.char_index_dic['%']) \n",
    "        \n",
    "        self.Dropout = nn.Dropout(self.dropout / 2)\n",
    "        \n",
    "        #self.bigru = nn.GRU(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=self.num_layers, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.BILSTM = nn.LSTM(input_size=self.embedding_size, hidden_size=self.hidden_size, num_layers=self.num_layers, bidirectional=True, batch_first=True, dropout = self.dropout)\n",
    "\n",
    "        \n",
    "        #self.gru = nn.GRU(input_size= self.embedding_size ,hidden_size = self.hidden_size * 2, num_layers = self.num_layers, batch_first = True)\n",
    "        self.LSTM = nn.LSTM(input_size= self.embedding_size ,hidden_size = self.hidden_size*2, num_layers = self.num_layers, batch_first = True , dropout = self.dropout)\n",
    "                \n",
    "        self.criterion = nn.CrossEntropyLoss(ignore_index =self.char_index_dic['%'])\n",
    "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
    "        self.Linear = nn.Linear(self.hidden_size * 2,len(self.vocab))\n",
    "        #self.Linear = nn.Linear(self.hidden_size * 2,1)\n",
    "        \n",
    "        #self.optimizer = optim.Adam([*self.BILSTM.parameters(), *self.LSTM.parameters()], lr = 0.1)\n",
    "        #self.optimizer = optim.AdamW(self.parameters(), lr = self.lr)\n",
    "        #self.optimizer = optim.RMSprop([*self.LSTM.parameters(), *self.BILSTM.parameters()], lr = self.lr)\n",
    "        #self.optimizer = optim.Adamax([*self.LSTM.parameters(), *self.BILSTM.parameters()], lr=self.lr)\n",
    "        #self.opt1 = optim.RMSprop(self.BILSTM.parameters(), lr = self.lr)\n",
    "\n",
    "        self.opt1 = optim.Adam(self.BILSTM.parameters(), lr = self.lr )\n",
    "        self.opt2 = optim.Adam([*self.LSTM.parameters(), *self.input_dense.parameters()], lr = self.lr)\n",
    "    \n",
    "    \n",
    "    def prepare_data(self, data):\n",
    "    \n",
    "        #Le'ts create a padding for ouriinstances : \n",
    "\n",
    "        pad_char = '%'\n",
    "        padded_data = []\n",
    "        ls_words = []\n",
    "        ls_roots = []\n",
    "        for instance in data : \n",
    "            ls_words.append(instance[0])\n",
    "            ls_roots.append(instance[1])\n",
    "        \n",
    "        # Let's calculate the biggest length\n",
    "        max_len_words = max([len(item) for item in ls_words])\n",
    "        max_len_roots = max([len(item) for item in ls_roots])\n",
    "\n",
    "        # Now we pad the word until we reach the max length\n",
    "        for instance in data: \n",
    "            tmp = []\n",
    "            word,root = instance[0], instance[1]\n",
    "            while(len(word) != max_len_words):\n",
    "                word += pad_char\n",
    "            tmp.append(word)\n",
    "            while(len(root) != max_len_roots):\n",
    "                root += pad_char\n",
    "            tmp.append(root)\n",
    "            padded_data.append(tmp)\n",
    "\n",
    "        # let's create our vocab : \n",
    "\n",
    "        vocab = []\n",
    "        for word in padded_data :\n",
    "            for item in word : \n",
    "                tmp = set(item)\n",
    "                for k in tmp : \n",
    "                    if k not in vocab : \n",
    "                        vocab.append(k)\n",
    "\n",
    "        # Let's create our dictionnary with unique indexes\n",
    "\n",
    "        char_to_idx_map = {char: idx for idx, char in enumerate(vocab)}\n",
    "\n",
    "        # Let's now split our data to batches\n",
    "\n",
    "        final_data = []\n",
    "        for instance in padded_data : \n",
    "            tmp = []\n",
    "            word = [char_to_idx_map[char] for char in instance[0]]\n",
    "            root = [char_to_idx_map[char] for char in instance[1]]\n",
    "            tmp.append(word)\n",
    "            tmp.append(root)\n",
    "            final_data.append(tmp)\n",
    "\n",
    "        size= self.batch_size \n",
    "        batches = [final_data[i:i + size] for i in range(0, len(final_data), size)]\n",
    "        \n",
    "        return batches , vocab , char_to_idx_map\n",
    "    \n",
    "    \n",
    "    def word_to_seq(self, word):\n",
    "        '''\n",
    "        this function returns a sequence of the unique indexes for the given word \n",
    "        (sequence is tensor that can be changed using a .tolist() )\n",
    "        '''\n",
    "        word_char_idx_seq =[self.char_index_dic[char] for char in word]    \n",
    "        return word_char_idx_seq # word sequence\n",
    "    \n",
    "    \n",
    "    # Let's now construct our model : \n",
    "    \n",
    "    # we should think about character embeddings in order to create an embeded matrix for each word\n",
    "        \n",
    "    \n",
    "    \n",
    "    def encode(self, batch):    \n",
    "        '''\n",
    "        input : a batch of sequences of instances : [word_seq , root_seq] * batch_size\n",
    "                input_size : (input_size,2)\n",
    "        '''\n",
    "        \n",
    "        word_batch = [] # list of words in the batch\n",
    "        root_batch = [] # list of roots in the batch\n",
    "        \n",
    "        for instance in batch : \n",
    "            word_batch.append(instance[0])\n",
    "            root_batch.append(instance[1])\n",
    "            \n",
    "        word_batch = torch.tensor(word_batch)\n",
    "        root_batch = torch.tensor(root_batch)\n",
    "        \n",
    "        # we create embedding of the word batch : \n",
    "        \n",
    "        embedded_word_batch = self.embedding(word_batch)\n",
    "        \n",
    "        \n",
    "        \n",
    "        init_hid = nn.init.xavier_normal_(torch.zeros(2*self.num_layers, len(batch), self.hidden_size), gain=0.5)\n",
    "        init_ce = nn.init.xavier_normal_(torch.zeros(2*self.num_layers, len(batch), self.hidden_size), gain=0.5)\n",
    "            \n",
    "        outputs, (hidden, cell) = self.BILSTM(embedded_word_batch, (init_hid, init_ce)) # we pass the emebedded vector through the bi-GRU \n",
    "    \n",
    "        # hidden size : [2 * num_layers, batch_size , hidden_size]\n",
    "        \n",
    "        # we want hidden size : [num_layers , batch_size  , 2 * hidden_size]\n",
    "        \n",
    "        # we return an adequate layer for the decoder : \n",
    "        \n",
    "        final_hid, final_ce = [], []\n",
    "        for k in range(0,hidden.size(0), 2):\n",
    "            \n",
    "            tmp_hid = hidden[k:k+2 , :, :]\n",
    "            tmp_ce = cell[k:k+2, :, :]\n",
    "            \n",
    "            \n",
    "            cct_hid = torch.cat((tmp_hid[0], tmp_hid[1]), dim  = 1).tolist()\n",
    "            cct_ce = torch.cat((tmp_ce[0], tmp_ce[1]), dim  = 1).tolist()\n",
    "            \n",
    "            final_hid.append(cct_hid)\n",
    "            final_ce.append(cct_ce)\n",
    "        \n",
    "        final_hid, final_ce = torch.tensor(final_hid), torch.tensor(final_ce)\n",
    "    \n",
    "        return root_batch , outputs ,(final_hid, final_ce)\n",
    "        \n",
    "    \n",
    "    def decode(self, encoder_outputs ,encoder_hidden_cell , batch, teacher_forcing_bool, epoch):\n",
    "        \n",
    "        '''\n",
    "        input : encoding_hidden_layer => corresponds to the concatenation of the final hidden layers \n",
    "                                        of the bidirectionnal gru in our encoder\n",
    "                \n",
    "                batch : subset of data that contains the roots of the words we encoded.\n",
    "                \n",
    "        output : we'll see :) \n",
    "        \n",
    "        '''\n",
    "\n",
    "        (hidden_layer , cell) , root_batch = encoder_hidden_cell , batch \n",
    "                        \n",
    "        embedded_char = self.embedding(torch.unsqueeze(root_batch[:, 0], 1))\n",
    "            \n",
    "        outputs = []\n",
    "        \n",
    "        #topk_indexes = []\n",
    "        \n",
    "        for i in range(root_batch.size(1)): \n",
    "            \n",
    "            self.Dropout(embedded_char)\n",
    "            \n",
    "            decoder_output , (hidden_layer, cell) = self.LSTM(embedded_char, (hidden_layer, cell))\n",
    "                        \n",
    "            # Let's calculate the scores  :\n",
    "\n",
    "            input_decoder_output = self.input_dense(decoder_output)\n",
    "            \n",
    "            embedded_char = input_decoder_output\n",
    "    \n",
    "            mask = np.where([random.random() <= (self.teacher_forcing_ratio) for i in range(root_batch.size(0))])[0]\n",
    "            \n",
    "            teacher_forcing_input = self.embedding(torch.unsqueeze(torch.clone(root_batch[:, i]), 1))\n",
    "            \n",
    "            if teacher_forcing_bool : \n",
    "\n",
    "                embedded_char[mask] = teacher_forcing_input[mask] \n",
    "                \n",
    "            Dense_decoded_output = self.Linear(decoder_output)\n",
    "            \n",
    "            soft = nn.Softmax(dim = 2)\n",
    "            \n",
    "            soft_out = soft(Dense_decoded_output)\n",
    "\n",
    "            #tst = torch.squeeze(soft_out, 1)\n",
    "            \n",
    "            #[128, 39]\n",
    "            \n",
    "            #tmp = torch.topk(tst, 3, dim = 1).tolist()\n",
    "            \n",
    "            #topk_indexes.append(tmp)\n",
    "            \n",
    "            outputs.append(soft_out)\n",
    "            \n",
    "            \n",
    "        return outputs \n",
    "                            \n",
    "        \n",
    "    \n",
    "    def train_model(self, batches, teacher_forcing_bool, epoch):\n",
    "                \n",
    "        train_batches = batches        \n",
    "         \n",
    "        epoch_loss = 0\n",
    "        \n",
    "        n = 0            \n",
    "                \n",
    "        test_word = '$' + 'ØªØ­ÙÙÙ' + 'Â£'\n",
    "        \n",
    "        for batch in train_batches :\n",
    "            \n",
    "            #print(self.predict(test_word))\n",
    "        \n",
    "            self.opt1.zero_grad()\n",
    "            self.opt2.zero_grad()\n",
    "\n",
    "            root_batch, encoder_output, encoder_states = self.encode(batch)\n",
    "\n",
    "            outputs = self.decode(encoder_output,encoder_states, root_batch, teacher_forcing_bool, epoch)\n",
    "\n",
    "            a = [torch.squeeze(item, 1) for item in outputs]\n",
    "            a = [torch.unsqueeze(item, 0) for item in a]\n",
    "\n",
    "            output = torch.cat(a, dim = 0)\n",
    "                        \n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output.view(-1, output_dim)\n",
    "            \n",
    "            trg = root_batch.transpose(0, 1)\n",
    "    \n",
    "            trg = trg.reshape(-1)\n",
    "        \n",
    "            loss = self.criterion(output, trg)\n",
    "        \n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_([*self.LSTM.parameters(), *self.BILSTM.parameters()], 1)\n",
    "\n",
    "            self.opt1.step()\n",
    "            self.opt2.step()\n",
    "            \n",
    "            #self.optimizer.step()\n",
    "\n",
    "            epoch_loss+=loss.item()\n",
    "\n",
    "            n+=1\n",
    "\n",
    "            print('the loss of the train batch ', n ,' is : ', loss.item())\n",
    "    \n",
    "        return epoch_loss/n\n",
    "\n",
    "    def evaluate_model(self, batches, teacher_forcing_bool, epoch):\n",
    "        '''\n",
    "        this method evaluates our model :=)\n",
    "        will be similar to train but without the teacher forcing/ using an optimizer \n",
    "        '''          \n",
    "        self.eval()\n",
    "\n",
    "        val_batches = batches\n",
    "\n",
    "        n = 0\n",
    "\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        with torch.no_grad() :\n",
    "\n",
    "            for batch in val_batches :\n",
    "\n",
    "                root_batch, encoder_output ,encoder_states = self.encode(batch)\n",
    "\n",
    "                outputs = self.decode(encoder_output ,encoder_states, root_batch, teacher_forcing_bool, epoch)\n",
    "\n",
    "                a = [torch.squeeze(item, 1) for item in outputs]\n",
    "                a = [torch.unsqueeze(item, 0) for item in a]\n",
    "\n",
    "                output = torch.cat(a, dim = 0)\n",
    "\n",
    "                output_dim = output.shape[-1]\n",
    "\n",
    "                output = output.view(-1, output_dim)\n",
    "\n",
    "                trg = root_batch.transpose(0, 1)\n",
    "\n",
    "                trg = trg.reshape(-1)\n",
    "                \n",
    "                #print(output.size(), trg.size())\n",
    "                \n",
    "                loss = self.criterion(output, trg)\n",
    "\n",
    "                epoch_loss+=loss.item()\n",
    "\n",
    "                n+=1\n",
    "\n",
    "                print('the loss of the val batch ', n ,' is : ', loss.item())\n",
    "\n",
    "        return epoch_loss / n\n",
    "    \n",
    "    def predict(self, word):\n",
    "        '''\n",
    "        this is the adaptation of encoder-decoder network on a single word w/o optimization\n",
    "        '''\n",
    "        \n",
    "        # Let's turn the word into a sequence of word indexes \n",
    "        word_seq = self.word_to_seq(word)\n",
    "\n",
    "        # Let's create an embedding of the word seq\n",
    "        embedded_word = self.embedding(torch.tensor(word_seq))\n",
    "\n",
    "        \n",
    "        init_hid = nn.init.xavier_normal_(torch.zeros(2*self.num_layers, self.hidden_size), gain=0.5)\n",
    "        init_ce = nn.init.xavier_normal_(torch.zeros(2*self.num_layers, self.hidden_size), gain=0.5)\n",
    "        # Let's feed our word embedding to the encoder network\n",
    "        outputs, (hidden, cell) = self.BILSTM(embedded_word, (init_hid, init_ce))\n",
    "        \n",
    "        #print(hidden.size())\n",
    "        \n",
    "        final_hid, final_ce = [], []\n",
    "        for k in range(0,hidden.size(0), 2):\n",
    "            \n",
    "            tmp_hid = hidden[k:k+2 ,:]\n",
    "            tmp_ce = cell[k:k+2, :]\n",
    "\n",
    "            cct_hid = torch.cat((tmp_hid[0], tmp_hid[1]), dim  = -1).tolist()\n",
    "            cct_ce = torch.cat((tmp_ce[0], tmp_ce[1]), dim  = -1).tolist()\n",
    "\n",
    "            final_hid.append(cct_hid)\n",
    "            final_ce.append(cct_ce)\n",
    "        \n",
    "        final_hidden, final_cell = torch.tensor(final_hid), torch.tensor(final_ce)\n",
    "\n",
    "        #initialize the input of the decoder\n",
    "\n",
    "        embedded_char = torch.unsqueeze(self.embedding(torch.tensor(self.char_index_dic[self.sow])), 0)\n",
    "\n",
    "        prediction_output = [] # a list of the outputs of the decoder \n",
    "     \n",
    "        # we create a softmax layer : \n",
    "\n",
    "        soft = nn.Softmax(dim = 1)\n",
    "        \n",
    "        key_list = list(self.char_index_dic.keys())\n",
    "        val_list = list(self.char_index_dic.values())\n",
    "        \n",
    "        for i in range(5):\n",
    "                        \n",
    "            decoder_output , (final_hidden, final_cell) = self.LSTM(embedded_char, (final_hidden, final_cell))\n",
    "\n",
    "            input_dense = nn.Linear(self.hidden_size * 2,self.embedding_size)\n",
    "            input_decoder_output = input_dense(decoder_output)\n",
    "\n",
    "            embedded_char = input_decoder_output\n",
    "\n",
    "            Dense_decoded_output = self.Linear(decoder_output)\n",
    "            prediction_output.append(soft(Dense_decoded_output).tolist())\n",
    "\n",
    "        prediction_output = torch.squeeze(torch.tensor(prediction_output), 1)\n",
    "        \n",
    "        #print(prediction_output.size())\n",
    "        \n",
    "        test_word_seq = word_seq[1:]\n",
    "        test_word_seq = test_word_seq[:-1]\n",
    "        \n",
    "        precision = 5\n",
    "        \n",
    "        top_idx = torch.topk(prediction_output, precision, dim = 1).indices\n",
    "\n",
    "        \n",
    "        init_char = 0\n",
    "        final_char = 0        \n",
    "        \n",
    "        \n",
    "        init_char = self.char_index_dic[self.sow]\n",
    "    \n",
    "        final_char = self.char_index_dic[self.eow]\n",
    "       \n",
    "        \n",
    "        grid = []\n",
    "        \n",
    "        for i in range(precision): \n",
    "            for j in range(precision):\n",
    "                for k in range(precision):\n",
    "                    tmp = []\n",
    "                    tmp.append((top_idx[1][i]).item())\n",
    "                    tmp.append((top_idx[2][j]).item())\n",
    "                    tmp.append((top_idx[3][k]).item())\n",
    "                    grid.append(tmp)\n",
    "        \n",
    "        # we check the possibilities : \n",
    "        \n",
    "        best_cases = []\n",
    "        \n",
    "        print(grid)\n",
    "        \n",
    "        for case in grid : \n",
    "            \n",
    "            s = [item for item in case if item in set(test_word_seq)] # we select elts from a that are in l \n",
    "            b = [item for item in test_word_seq if item in set(s)] # \n",
    "            \n",
    "            #print(s, b)\n",
    "            \n",
    "            if s == b and s != [] : \n",
    "                best_cases.append(case)\n",
    "            \n",
    "        \n",
    "        \n",
    "        # potential roots : \n",
    "        \n",
    "        pot_seq = []\n",
    "        \n",
    "        for item in best_cases : \n",
    "            \n",
    "            tmp =  item  \n",
    "            if (\"$\" not in tmp)  and (\"Â£\" not in tmp) :\n",
    "                pot_seq.append(tmp)           \n",
    "            \n",
    "\n",
    "        #best_char_indexes = [torch.argmax(item).item() for item in prediction_output]\n",
    "        \n",
    "        #t = torch.squeeze(torch.tensor(prediction_output), 1)\n",
    "        \n",
    "        #topk_out =  torch.topk(t,3,  dim = 1).indices.tolist()\n",
    "        \n",
    "        final_roots =[]\n",
    "        \n",
    "        for seq in pot_seq : \n",
    "            \n",
    "            position = [val_list.index(item) for item in seq]\n",
    "\n",
    "            result_char = [key_list[pos] for pos in position]\n",
    "            predicted_root = ''.join(result_char)\n",
    "            final_roots.append(predicted_root)\n",
    "\n",
    "            \n",
    "    \n",
    "        return final_roots\n",
    "\n",
    "    \n",
    "\n",
    "    def fit(self, num_epochs):\n",
    "        \n",
    "        \"\"\"\n",
    "        let's first prepare our data\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        print(f'The model has {self.count_parameters():,} trainable parameters')\n",
    "        \n",
    "        data = self.data\n",
    "        \n",
    "        data = random.sample(data, len(data))\n",
    "        data_size = len(data)\n",
    "        middle_index = int(data_size * self.ratio)        \n",
    "        train_data , val_data = data[:middle_index], data[middle_index:]\n",
    "        \n",
    "        train_batches, voc, dic = self.prepare_data(train_data)\n",
    "        val_batches ,voc , dic = self.prepare_data(val_data)\n",
    "        \n",
    "        epochs = list(range(num_epochs))\n",
    "        \n",
    "        best_val_loss = 1000\n",
    "        best_model_par = 0\n",
    "        \n",
    "        losses =[]\n",
    "        predicted_roots = []\n",
    "        test_word = '$' + 'ØªØ­ÙÙÙ' + 'Â£'\n",
    " \n",
    "        for epoch in epochs : \n",
    "                \n",
    "            print('epoch num : ', epoch) \n",
    "            print(self.char_index_dic)\n",
    "\n",
    "            \n",
    "            \n",
    "            t1 = time.time()\n",
    "            \n",
    "            train_batches = random.sample(train_batches , len(train_batches))\n",
    "            #val_batches = random.sample(val_batches, len(val_batches))\n",
    "                        \n",
    "            train_loss= self.train_model(train_batches, 1, epoch)\n",
    "            val_loss = self.evaluate_model(val_batches, 0, epoch) # we set the teacher forcing to false            \n",
    "            t2 = time.time()\n",
    "            \n",
    "            predicted_root = self.predict(test_word)\n",
    "            print(predicted_root)\n",
    "            predicted_roots.append(predicted_root)\n",
    "            \n",
    "            \n",
    "            \n",
    "            tmp = [train_loss, val_loss]\n",
    "            losses.append(tmp)\n",
    "            \n",
    "            print('the training loss : ', train_loss , 'the val loss :', val_loss)\n",
    "            print('epoch num : ' ,epoch , ' lasted : ', t2 - t1 , 'seconds')\n",
    "            \n",
    "            if val_loss < best_val_loss :\n",
    "                \n",
    "                best_val_loss = val_loss \n",
    "                best_model_par = self.state_dict()\n",
    "\n",
    "            \n",
    "        torch.save(best_model_par, 'best_model.pt')\n",
    "            \n",
    "        return losses\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        '''\n",
    "        function to calculate the total number of parameters in the model\n",
    "        '''\n",
    "        return sum(torch.numel(p) for p in self.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "203f2ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'corpus_morphological_analysis'\n",
    "file_paths = []\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    file_paths.append(f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44d5523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_file_paths = file_paths[:4000] # we take the first 5000 files out of 29000 files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47462b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifie le prefixe, racine et le suffixe et les placent dans un dictionnaire\n",
    "def identify(word_l):\n",
    "    if len(word_l) < 4 :\n",
    "        return None\n",
    "    dictt = {}\n",
    "    dictt['word'] = word_l[0]\n",
    "    # le cas s'il existe un prÃ©fixe\n",
    "    if word_l[2] != '' and word_l[2] != ' ' :   \n",
    "        if word_l[4] not in word_l[0]: \n",
    "            if word_l[5] in word_l[0] and word_l[5] != '': \n",
    "                dictt['prefixe'] = word_l[2]\n",
    "                dictt['root'] = word_l[8]\n",
    "                dictt['suffixe'] = word_l[9]\n",
    "            elif word_l[3] in word_l[0] and word_l[3] != '':\n",
    "                dictt['prefixe'] = word_l[2]\n",
    "                dictt['root'] = word_l[3]\n",
    "                dictt['suffixe'] = ''\n",
    "        else :\n",
    "            dictt['prefixe'] = word_l[2]\n",
    "            dictt['root'] = word_l[7]\n",
    "            dictt['suffixe'] = word_l[8]\n",
    "    # s'il n'existe pas un prÃ©fixe\n",
    "    else : \n",
    "        if word_l[2] == '' : \n",
    "            dictt['prefixe'] = word_l[2]\n",
    "            dictt['root'] = word_l[6]\n",
    "            dictt['suffixe'] = word_l[7]\n",
    "        elif  word_l[2] == ' ' :\n",
    "            dictt['prefixe'] = ''\n",
    "            dictt['root'] = word_l[3]\n",
    "            dictt['suffixe'] = ''    \n",
    "    return dictt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "725b4b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = []\n",
    "#i = 0\n",
    "for filepath in temp_file_paths :\n",
    "    #print(i)\n",
    "    with open(filepath, encoding='utf-8') as f :\n",
    "        html = f.read()\n",
    "    soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()  \n",
    "    text = soup.get_text()\n",
    "    content.append(text)\n",
    "    #i+=1\n",
    "split_list = []\n",
    "for item in content : \n",
    "    tmp = item.splitlines()\n",
    "    split_list.append(tmp)\n",
    "work_list = []\n",
    "for k in split_list :\n",
    "    l = [item for item in k if 'ÙØ§ ØªÙØ¬Ø¯ ÙØªØ§Ø¦Ø¬ ÙØªØ­ÙÙÙ ÙØ°Ù Ø§ÙÙÙÙØ©' not in item]\n",
    "    tmp_l = [item.replace(\"#\",'') for item in l]\n",
    "    work_list.append(tmp_l)\n",
    "final_list = []\n",
    "for k in work_list :\n",
    "    tst = [item.split(':') for item in k]\n",
    "    final_list.append(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89eebb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtre la liste de mots en liste de dictionnaires\n",
    "def word_to_dict_list(wordlist):\n",
    "    dictlist = []\n",
    "    for k in wordlist : \n",
    "        dictlist.append(identify(k))\n",
    "    return dictlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00808de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = []\n",
    "for k in final_list: \n",
    "    for j in k :\n",
    "        s = identify(j)\n",
    "        if s == None :\n",
    "            continue\n",
    "        final.append(identify(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afaf8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dic_to_list(listt):\n",
    "    L = []\n",
    "    for k in listt : \n",
    "        tmp = []\n",
    "        #print(k)\n",
    "        if len(k) == 4 : \n",
    "            tmp.append(k['word'])\n",
    "            tmp.append(k['prefixe'])\n",
    "            tmp.append(k['root'])\n",
    "            tmp.append(k['suffixe'])\n",
    "            L.append(tmp)\n",
    "    return L\n",
    "data = dic_to_list(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d96530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_l = dic_to_list(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b930f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_data = []\n",
    "for word in data : \n",
    "    tmp =[]\n",
    "    tmp.append(word[0])\n",
    "    tmp.append(word[2])\n",
    "    root_data.append(tmp)\n",
    "#root_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15f4e1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = []\n",
    "for item in root_data : \n",
    "    tmp = []\n",
    "    if len(item[1]) <= 3 and len(item[1]) != 0:\n",
    "        tmp.append('$'+item[0]+'Â£')\n",
    "        tmp.append('$'+item[1]+'Â£')\n",
    "        data_root.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2276b228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563071\n",
      "1563071\n",
      "1363580\n"
     ]
    }
   ],
   "source": [
    "print(len(data_root))\n",
    "for item in data_root :\n",
    "    if len(item[0])==15 or len(item[0])==16:\n",
    "        data_root.pop(data_root.index(item))\n",
    "print(len(data_root))\n",
    "d = []\n",
    "for item in data_root:\n",
    "    if len(item[0]) > 4 :\n",
    "        d.append(item)\n",
    "print(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19389d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = model(d[:10000], 512, 64 , 100 , 3 , 0.2 ,0.35, 0.0005)\n",
    "test_model.load_state_dict(torch.load('best_model.pt'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ff86cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 18, 18], [3, 18, 22], [3, 18, 0], [3, 18, 7], [3, 18, 19], [3, 22, 18], [3, 22, 22], [3, 22, 0], [3, 22, 7], [3, 22, 19], [3, 0, 18], [3, 0, 22], [3, 0, 0], [3, 0, 7], [3, 0, 19], [3, 3, 18], [3, 3, 22], [3, 3, 0], [3, 3, 7], [3, 3, 19], [3, 7, 18], [3, 7, 22], [3, 7, 0], [3, 7, 7], [3, 7, 19], [1, 18, 18], [1, 18, 22], [1, 18, 0], [1, 18, 7], [1, 18, 19], [1, 22, 18], [1, 22, 22], [1, 22, 0], [1, 22, 7], [1, 22, 19], [1, 0, 18], [1, 0, 22], [1, 0, 0], [1, 0, 7], [1, 0, 19], [1, 3, 18], [1, 3, 22], [1, 3, 0], [1, 3, 7], [1, 3, 19], [1, 7, 18], [1, 7, 22], [1, 7, 0], [1, 7, 7], [1, 7, 19], [22, 18, 18], [22, 18, 22], [22, 18, 0], [22, 18, 7], [22, 18, 19], [22, 22, 18], [22, 22, 22], [22, 22, 0], [22, 22, 7], [22, 22, 19], [22, 0, 18], [22, 0, 22], [22, 0, 0], [22, 0, 7], [22, 0, 19], [22, 3, 18], [22, 3, 22], [22, 3, 0], [22, 3, 7], [22, 3, 19], [22, 7, 18], [22, 7, 22], [22, 7, 0], [22, 7, 7], [22, 7, 19], [0, 18, 18], [0, 18, 22], [0, 18, 0], [0, 18, 7], [0, 18, 19], [0, 22, 18], [0, 22, 22], [0, 22, 0], [0, 22, 7], [0, 22, 19], [0, 0, 18], [0, 0, 22], [0, 0, 0], [0, 0, 7], [0, 0, 19], [0, 3, 18], [0, 3, 22], [0, 3, 0], [0, 3, 7], [0, 3, 19], [0, 7, 18], [0, 7, 22], [0, 7, 0], [0, 7, 7], [0, 7, 19], [20, 18, 18], [20, 18, 22], [20, 18, 0], [20, 18, 7], [20, 18, 19], [20, 22, 18], [20, 22, 22], [20, 22, 0], [20, 22, 7], [20, 22, 19], [20, 0, 18], [20, 0, 22], [20, 0, 0], [20, 0, 7], [20, 0, 19], [20, 3, 18], [20, 3, 22], [20, 3, 0], [20, 3, 7], [20, 3, 19], [20, 7, 18], [20, 7, 22], [20, 7, 0], [20, 7, 7], [20, 7, 19]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ØªØ©Ø©',\n",
       " 'ØªØ©Ù',\n",
       " 'ØªØ©Ù',\n",
       " 'ØªØ©Ø´',\n",
       " 'ØªÙØ©',\n",
       " 'ØªÙÙ',\n",
       " 'ØªÙÙ',\n",
       " 'ØªÙØ´',\n",
       " 'ØªÙØ©',\n",
       " 'ØªÙÙ',\n",
       " 'ØªÙÙ',\n",
       " 'ØªÙØ´',\n",
       " 'ÙÙÙ',\n",
       " 'ÙÙÙ',\n",
       " 'ÙÙÙ']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = '$' + 'ÙØ³ØªØ®Ø¯Ù' + 'Â£'\n",
    "test = test_model.predict(word)\n",
    "test_final = []\n",
    "for w in test :\n",
    "    if (\"$\" not in w ) and (\"Â£\" not in w) :\n",
    "       test_final.append(w)\n",
    "test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d2bb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
